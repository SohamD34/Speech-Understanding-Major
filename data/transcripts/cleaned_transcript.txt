We have been talking about this audio processing with respect to speaker recognition, speech recognition or any such related task. But you also know that with any of these security technologies, there can be attacks or there can be people who have any kind of ill intention who would to defraud the system. Right? Who would to fool the system? Have you heard of any such examples, any such real-world examples where any kind of security system is in place, be it biometrics, face, voice, any of those. And there have been cases where these systems have been fooled. Anybody remembers any such instance? Would to share. And when you have to speak up. Is he going to be on the... Okay, all right. So there have been several such instances, not only in... I'm audible, right? Somebody please speak up. I'm not able to hear you guys. Yes, please. To see the text and all. Okay, so there have been several such instances with respect to different kinds of tasks, automation tasks that we have been performing. Right? And specifically speech. These are all the news clippings that you see of documented cases. Okay? One of the interesting ones, the one that you see at the bottom, city bank launches, voice passwords in India. Right? So city bank launched voice passwords. You can gain access to accounts by using... By speaking out your passwords. And you will also see in capture, right? That you can speak out what... So there are image captures and then there are audio captures also that are there. For people with special requirements, they can speak out audio captures, right? So... But these can be fooled using some of these technologies. And this is what also happened in some of these cases. For example, this adoboco, they have Photoshop for voice which can lead to concerns. We have robot speech simulator and several other such things that have been proposed in the literature. And similar counterparts have also been proposed which can be used to defraud. Okay? So specifically speaking, this line of research of presentation attack we call it or spoofing, this has been in the community for 15 years or so now. The research started the first competition around this. It's called as the spoofing audio spoofing competition. AASB audio spoofing verification competition we call it. So this competition started I think around 2000 again or so in the research community, right? At that time, they were looking at some very specific kinds of attacks and I'll show you what those kinds of attacks were. But these competitions, when they're still running, those competitions are still running to evaluate the performance or of these attack detection algorithms or manipulation detection algorithms. But after few years that these research on presentation of spoofing started, ISO got involved. This international standards organization and they created standards document on presentation attacks. Earlier we used to call it spoofing but then I also termed the test presentation attacks. So anything that is presented in front of biometric system, it can be attacked. So and then there are different kinds of attacks. So the earlier the nomenclature used to be real spoof but then they proposed this nomenclature of bonafide and imposter. So bonafide is real one. So in this case with respect to face, if you see bonafide is the real face. So you would enroll real face. real face is coming as query. So this is predicted as genuine, right? But then you could have imposter. Now what are those imposter? Imposter is anyone who's trying to fraudulently get access to the system. Now this is something called as zero effort imposter. What is zero effort imposter? So let's say I want to gain access to your account, right? I just go in front of the biometric system and I give my biometric. Let's say that biometric is face or that biometric is audio. I just give that biometric as I just without any effort of trying to look you or trying to impersonate your voice. Or fingerprint whatever that modality is, I just go using my characteristics and try to gain access to your account, right? So that is something called zero effort imposter which is in which case it is very likely that I will not be able to gain access. It will be it will be filled attempt, right? But then there are cases in which one can get access and go back to dodging to zoom. When where we had these goat lamb sheep's and holes, right? There are some cases, there's some individuals who are easy to impersonate. So if these are easy to impersonate people, even with zero effort imposter, I might be I might just be able to gain access depending on. Depending on lot of characteristics, lot of factors, right? But then what I can also do is do bit more effort in trying to gain access to your account. Now in what ways can I try to gain access to your account? Let's say if it is audio in what ways can I get access to your account? Can you think of it? If I have to spoof an audio data and audio signal, if you were given this task of spoofing, what will you do? How will you create spoof of audio? Let's say you have to gain access to my account. What will you do? I'm waiting for the answers. Try to try to speak in your language my language. Okay, so try to speak in my audio. Yes, nearly, master is spoofing in your form why? Okay, anything else? Record your voice and then feed it. Record my voice and feed it. That is good. What else? Use the text-based speech model and text to my name is Richabh, please open the account, just type of thing so that the model creates an audio and she gets in the network. We can also make deep break of the voice. You can also make deep break of my voice. All of these are valid answers. Let me just play this audio signals for you in order to play this. I think I'll have to go to the drive. Let me go to the drive. Let me go to the drive. I'll go to the drive. Okay, so whose voice is this? Charu Khan, obviously. Do you think it is real voice of Charu Khan or this is an end of AI attack? You cannot hear the audio. You cannot hear my audio or you cannot hear the audio. Videos, that audio. You couldn't hear the audio. I played right? Yes ma'am. Okay, let me give me once again. Let me stop presenting. In your now? I'll start with the next one. Yes ma'am. Sitri Vinayek Electronics, let us call the smartphone. Mark the server. Yes, I'll do it. Okay, it is in rated sound. It is in rated sound. Okay, some gaps in Charu Khan's voice, there is some gap. He is used to speaking with gaps. No? Yes, yes. Anybody else would to give try? Okay, let me play another one. I can only say that I am good audience. What about this? Yes ma'am. I think real voice. Okay. No, I don't think it is real. The tonality is little bit changed. Okay, so both of these guys are generated audios. Okay, Charu Khan as well as Amarish Puri, this Amarish Puri ka audio was generated after he passed away. And this, Charu Khan's audio, so there is this entire video that was generated few years back by Katbury's. And they did , they did , add with Charu Khan's voice for local Kirana stores. Right, because if you hear the audio, he is talking about local Kirana's store, this one and then pass the shop of Sharmaji, this one and all of that. These guys can't afford Charu Khan, these local Kirana stores and local Angle and Antishops. They can't afford Charu Khan. So they, Katbury's used Genie I, I think it was Katbury's, they used Genie I to create these, create this entire add. And that add, you can find that add on YouTube, it should be still there. So benefit is you can create, but then I don't know if they took permission from Charu Khan or not. Right, you can, you can create all those sort of things. And I mean, you guys know that we are talking about B-Fix, still we are having discussion on whether it is fake or not. Think about any normal person who does not know the concept of B-Fix so well, that how good or how bad it is, how would they differentiate between the two. Right. So there is an entire categorization of spoofing attacks. You can generate samples based on AI methodologies and using some non AI methodologies as per. So if we talk about, obviously these non AI methodologies came earlier and this when I was talking about the research started 15 years back, we started with non AI methodologies, right. And non AI methodologies, therefore 5 kinds of attacks which are more common ones. The first one is your replay, right. So replays I think Ravi's, Ravi suggested it earlier that what you can do is you have my, you have my audio right now that I have, you have so many required audio samples of me from over multiple classes, right. If you want to gain access to my account, you can, you can take this audio and you can replay it whenever, take out words or do something of that sort and replay it to gain access to my account, right. So that is your replay attack, okay. To give you an example of replay with respect to face, this is how you can actually visualize it. So this is your real real, real frames from real video. These are again your frames from real video. But what you can do is you can record this video and replay it. Let's say I was trying to gain access to my KYC or my account or I want to be present in meeting, put attendance using face biometric system, you can use that and gain access multiple times using that, right. So again, several years back, okay, there are these school teachers in some country, I think it was Pakistan. So there was this grant coming in from United Nations for school teachers. And they were mandated to put attendance using face biometrics because there was grant coming in for promoting school education in different countries. So they said that we and and salaries of teachers and everybody was going in, they kind of mandated that in order to ensure that teachers go to school every day. So you have to go to school, you have to put your, you have to take selfie and you have to post it on some server with date and time stamp so that, your attendance is recorded, right. And based on that attendance, this already was released every month, right. So what some of the teachers used to do that what I'll do is today, I'll take picture, I'll go to school, I'll take five pictures for me, okay. And what I can do is tomorrow, if I want to skip school, I have multiple pictures, I'll take this picture and I'll click another picture of this from the phone, okay. Or I'll take printout of this, click another picture and upload this. So the date and time stamp that comes with the photo in the metadata that will be for tomorrow because I clicked the photograph from the photograph, the impersonation one that I had, that I clicked tomorrow. So then then date and time stamp will be fixed and I'll have my attendance marked, right. The same kind of thing can be done with respect to audio where you can capture these sentences and you can replay it again and again, right. So today I go with the background and all, I can capture the audio and wherever required, I can replay the audio to gain access or to provide incorrect information or whatever is the requirement, right. So that is your replay attack. impersonation is simple, we understand that if you try to impersonate someone, and this impersonation can be done by different methods. Earlier this used to be non-AI based and in non-AI based, it would be what would be non-AI based impersonation? It would be something , there are mimicry artists, right. There are mimicry artists who can very well mimic the audio of Shah Rukh Khan, Amitabh Bachchan or anybody, right. They can mimic anybody's voice. So that mimicry would be, that mimicry, if used for impersonation, would be considered as an attack and that would be, that would be impersonation to gain access, right. Another one is copy move, right. What is copy move? Copy move is something where I can take an audio signal and take some parts of an audio from one place to other in the audio, right. And this I think happened in the elections this time when we had general elections. When we had general elections this time, this is new clapping I got from YouTube and let me just play this for you. You can see the screen, the YouTube. No, I'm just screened not shared. Oh screen is not shared. Oh sorry sorry sorry. The screen got unshared. Okay, can you see it now? Yes. Man, not able to hear the voice. Oh, you are again not able to hear the voice. Why is it doing this? Anybody knows? I guess instead of sharing the screen, you could share the tab that would allow us to hear the audio in the YouTube tab. Okay. Let me do that entire screen. Room tab. Can you hear this now? Yes, yes. Okay, let me just play the first audio very quickly. Can you hear this now? Yes, yes. Okay, let me just play the first audio very quickly. See, this is the first audio which was claimed to be the original audio actually. Now let me play you the fake audio. See, what happened here? Lip is not thinking with voice. Lip is not voice. Okay. What is the kind of attack? What we are trying to do here? So what they did in this is they took some of the, they had the original video. They had the original video of Mr. Amit Shah. Right. And in some places, they didn't change the entire audio or the entire video. What they did? This in between, in the beginning of the speech, he used this word. They not they used. They inserted this word, gar samvedhane. Right. When he was referring to this, they inserted the word, gar samvedhane. And then towards the end, when he was saying that this reservation will continue or they said, they said, he is going to accept it. So in copy move what we do, you take word from somewhere and you insert it. Right. Or you can change phrases. You have phrase spoken at one place. You pick up this phrase and you insert it somewhere else. Now in this case, what they did was this, this gar samvedhane is probably the word which they might have found. They might not have found anywhere to be in the same speech. Right. Now if they did not find that word in the same speech with the same attire because there is video also. See, this is multi model spoofing that is happening. Because you have to spoof the video and you also have to spoof the audio along with it. So lip syncing and everything else has to go hand in hand very well. Right. So, if he had spoken the word gar samvedhane anywhere, anytime. You can pick up that word and insert it here. Right. But since you have to, you have to do complete multi model spoofing, multi model generation of the attack. You need him to speak. You need video of him speaking the same thing with lip syncing and everything. And that has to be inserted in the middle and those three barbocchi because the sentence has to flow in place. Right. Earlier, when we attacked this copy move, we used to do this. Used to be very simple copy paste here and there. Either cut copy paste, cut copy move is something we used to say. Right. So, you delete it or copy move it here. But now what has happened. And in the in earlier days, it used to be just. If you have an audio of the person speaking those sentences or words, only then you could create this. But today with this generative way, I algorithm, what you can do is you can also create these complete instances of people speaking. Right. Even if they even if they weren't existing with you. So, this is combination this Amitshaka video that you just saw. Amitshaka video plus audio that we saw. It was combination of copy move along with speech synthesis. Right. Along with speech synthesis that we were able to generate. So, not only speech synthesis, probably video synthesis also that happened along with it because. If speech synthesis is done, so the entire thing has to be shifted further back. Right. Because words are introduced. So, so that is what we did. Right. And then audio splicing is audio. You can use it to splice. You can actually so audio splicing would be similar to copy move where copy move you were just shifting out. Audio splicing you can actually remove out or words completely or phrases completely right. So, earlier again this used to be all non AI today. You can do all of it with AI and what you will get is. lot more simplified version and not simplified lot more smooth out version. Of the attacks right. So, replay. Carstakhtehe ho. Usay we can do replay with. Using AI based attacks you can do impersonation you can do copy move audio splice. Audio splicing using all of using AI generate via algorithms right. And in AI based the two main categories that you have is you can do speech synthesis. So, in speech synthesis what we do in speech synthesis we can have the complete speech synthesized right. With you and you can do it in who so ever voice you want. So, did I play you that song of Aate first slum and regime thing sometime in the past. You guys remember. You have to speak out I am not able to see the chat. I am time remember that there was no means to have had not made any song of Aate first. You don't remember okay I will search for it and I will I will show it to you okay. So, you could do speech synthesis so generate something completely in Amrish puri's audio right. So, that Amrish puri's audio voice complete synthesis. We just have we don't we had completely new generation of that audio right. And then you could do voice conversion and in voice conversion you can you can take whatever I am speaking. And you can have it present in let's say Shreya's voice or you can have it present in Shams voice. So, the complete lecture is being given by any other student be it Shambh, be it anybody else right. So, so the entire voice conversion can be done using .I. based attacks. Now, this this broad category of being able to impersonate or being able to create new things. Change from one to other this this broad line of attacks is called as deep fake attacks. Images you have seen lot of it in the in the general news and stuff. But this is very very much possible very much even prevalent I would say.