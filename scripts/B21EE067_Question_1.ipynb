{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzU7FJygea-J"
      },
      "source": [
        "*Note* - Before running this notebook, please make sure that there is a folder 'data' in the working '/' directory. The 'data' folder should contain a subdirectory 'video_lecture' with the 'speech_recording.mp4' file for the lecture video to be transcribed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yXqn4YhxRk-M"
      },
      "outputs": [],
      "source": [
        "!pip install openai-whisper deep-translator ffmpeg-python langdetect --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIEAprOPvN4z",
        "outputId": "cd3c9f3e-784e-4d7d-fb27-1f6460aa9f05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/parler-tts.git --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQrJ3LtNQqZq"
      },
      "source": [
        "## Transcription"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjjK4nhyHZUV",
        "outputId": "c9c5530a-77e2-4942-fc70-b680879f0454"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:parler_tts.modeling_parler_tts:Flash attention 2 is not installed\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import gdown\n",
        "import requests\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from parler_tts import ParlerTTSForConditionalGeneration\n",
        "from transformers import AutoTokenizer\n",
        "import soundfile as sf\n",
        "import warnings\n",
        "import ffmpeg\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQEBeCgtQqBJ",
        "outputId": "19303718-7d1b-4302-b75d-1c275f164678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Whisper base model...\n",
            "Transcribing speech_recording.mp4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Transcribing:   0%|          | 00:00\n",
            "  0%|          | 0/169072 [00:00<?, ?frames/s]\u001b[A\n",
            "  1%|          | 1400/169072 [00:01<03:57, 705.00frames/s]\u001b[A\n",
            "  2%|▏         | 3400/169072 [00:02<01:47, 1543.76frames/s]\u001b[A\n",
            "  4%|▍         | 6400/169072 [00:03<01:04, 2539.32frames/s]\u001b[A\n",
            "  6%|▌         | 9300/169072 [00:04<00:59, 2693.97frames/s]\u001b[A\n",
            "  7%|▋         | 11800/169072 [00:04<00:55, 2850.88frames/s]\u001b[A\n",
            "  9%|▊         | 14600/169072 [00:05<00:53, 2901.69frames/s]\u001b[A\n",
            " 10%|█         | 17500/169072 [00:06<00:53, 2853.08frames/s]\u001b[A\n",
            " 12%|█▏        | 20100/169072 [00:07<00:54, 2749.08frames/s]\u001b[A\n",
            " 13%|█▎        | 22500/169072 [00:08<00:47, 3054.61frames/s]\u001b[A\n",
            " 15%|█▍        | 25200/169072 [00:09<00:45, 3190.63frames/s]\u001b[A\n",
            " 17%|█▋        | 27900/169072 [00:10<00:45, 3098.02frames/s]\u001b[A\n",
            " 18%|█▊        | 30000/169072 [00:11<00:46, 2962.47frames/s]\u001b[A\n",
            " 19%|█▉        | 32700/169072 [00:11<00:41, 3309.34frames/s]\u001b[A\n",
            " 21%|██        | 35300/169072 [00:12<00:41, 3240.21frames/s]\u001b[A\n",
            " 22%|██▏       | 37900/169072 [00:13<00:38, 3394.61frames/s]\u001b[A\n",
            " 24%|██▎       | 40000/169072 [00:13<00:38, 3383.10frames/s]\u001b[A\n",
            " 25%|██▌       | 43000/169072 [00:14<00:33, 3781.66frames/s]\u001b[A\n",
            " 26%|██▋       | 44400/169072 [00:14<00:28, 4342.45frames/s]\u001b[A\n",
            " 28%|██▊       | 46500/169072 [00:14<00:25, 4752.37frames/s]\u001b[A\n",
            " 29%|██▉       | 49500/169072 [00:15<00:23, 5034.92frames/s]\u001b[A\n",
            " 30%|███       | 51300/169072 [00:15<00:21, 5529.51frames/s]\u001b[A\n",
            " 32%|███▏      | 54200/169072 [00:16<00:24, 4740.22frames/s]\u001b[A\n",
            " 34%|███▍      | 57100/169072 [00:16<00:22, 4966.17frames/s]\u001b[A\n",
            " 35%|███▌      | 59800/169072 [00:17<00:24, 4381.65frames/s]\u001b[A\n",
            " 37%|███▋      | 62600/169072 [00:18<00:29, 3626.55frames/s]\u001b[A\n",
            " 39%|███▊      | 65200/169072 [00:19<00:33, 3087.48frames/s]\u001b[A\n",
            " 40%|████      | 68100/169072 [00:20<00:28, 3489.71frames/s]\u001b[A\n",
            " 42%|████▏     | 70800/169072 [00:21<00:27, 3616.19frames/s]\u001b[A\n",
            " 44%|████▎     | 73600/169072 [00:21<00:25, 3708.54frames/s]\u001b[A\n",
            " 45%|████▌     | 76200/169072 [00:22<00:26, 3492.00frames/s]\u001b[A\n",
            " 47%|████▋     | 79100/169072 [00:23<00:28, 3144.87frames/s]\u001b[A\n",
            " 48%|████▊     | 81900/169072 [00:24<00:27, 3132.80frames/s]\u001b[A\n",
            " 50%|█████     | 84800/169072 [00:25<00:28, 3002.62frames/s]\u001b[A\n",
            " 52%|█████▏    | 87200/169072 [00:26<00:27, 3007.88frames/s]\u001b[A\n",
            " 53%|█████▎    | 90100/169072 [00:27<00:23, 3363.42frames/s]\u001b[A\n",
            " 55%|█████▍    | 92500/169072 [00:27<00:19, 3860.64frames/s]\u001b[A\n",
            " 56%|█████▌    | 95000/169072 [00:28<00:18, 3919.68frames/s]\u001b[A\n",
            " 58%|█████▊    | 97300/169072 [00:28<00:17, 4006.11frames/s]\u001b[A\n",
            " 59%|█████▉    | 99900/169072 [00:29<00:17, 4020.42frames/s]\u001b[A\n",
            " 61%|██████    | 102800/169072 [00:30<00:17, 3778.51frames/s]\u001b[A\n",
            " 62%|██████▏   | 105300/169072 [00:31<00:18, 3509.99frames/s]\u001b[A\n",
            " 64%|██████▍   | 108300/169072 [00:32<00:19, 3189.19frames/s]\u001b[A\n",
            " 66%|██████▌   | 111200/169072 [00:32<00:16, 3559.79frames/s]\u001b[A\n",
            " 67%|██████▋   | 113800/169072 [00:33<00:13, 3987.61frames/s]\u001b[A\n",
            " 68%|██████▊   | 115000/169072 [00:33<00:13, 4068.79frames/s]\u001b[A\n",
            " 70%|██████▉   | 118000/169072 [00:33<00:09, 5374.73frames/s]\u001b[A\n",
            " 70%|███████   | 118900/169072 [00:34<00:10, 4934.51frames/s]\u001b[A\n",
            " 71%|███████▏  | 120800/169072 [00:34<00:09, 5008.36frames/s]\u001b[A\n",
            " 73%|███████▎  | 123800/169072 [00:35<00:12, 3764.06frames/s]\u001b[A\n",
            " 74%|███████▍  | 125400/169072 [00:35<00:10, 4193.43frames/s]\u001b[A\n",
            " 76%|███████▌  | 127900/169072 [00:36<00:11, 3574.03frames/s]\u001b[A\n",
            " 77%|███████▋  | 130000/169072 [00:37<00:09, 4015.59frames/s]\u001b[A\n",
            " 78%|███████▊  | 132300/169072 [00:37<00:09, 4021.45frames/s]\u001b[A\n",
            " 80%|███████▉  | 135200/169072 [00:38<00:08, 3914.17frames/s]\u001b[A\n",
            " 81%|████████▏ | 137500/169072 [00:39<00:08, 3565.33frames/s]\u001b[A\n",
            " 83%|████████▎ | 139700/169072 [00:39<00:08, 3442.04frames/s]\u001b[A\n",
            " 84%|████████▍ | 142000/169072 [00:40<00:08, 3357.77frames/s]\u001b[A\n",
            " 85%|████████▌ | 144300/169072 [00:41<00:07, 3294.93frames/s]\u001b[A\n",
            " 87%|████████▋ | 147200/169072 [00:42<00:06, 3140.04frames/s]\u001b[A\n",
            " 89%|████████▊ | 149900/169072 [00:43<00:06, 2869.46frames/s]\u001b[A\n",
            " 90%|█████████ | 152300/169072 [00:44<00:05, 2847.84frames/s]\u001b[A\n",
            " 92%|█████████▏| 154900/169072 [00:44<00:04, 3107.93frames/s]\u001b[A\n",
            " 93%|█████████▎| 157500/169072 [00:45<00:03, 3448.31frames/s]\u001b[A\n",
            " 95%|█████████▍| 160000/169072 [00:45<00:02, 3899.96frames/s]\u001b[A\n",
            " 96%|█████████▌| 162600/169072 [00:46<00:01, 3756.10frames/s]\u001b[A\n",
            " 97%|█████████▋| 164700/169072 [00:47<00:01, 3746.77frames/s]\u001b[A\n",
            " 99%|█████████▊| 166800/169072 [00:47<00:00, 3753.68frames/s]\u001b[A\n",
            " 99%|█████████▉| 168000/169072 [00:48<00:00, 3752.36frames/s]\u001b[A\n",
            "100%|██████████| 169072/169072 [00:48<00:00, 3492.12frames/s]\n",
            "Transcribing: |          | 00:54"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " We have been talking about this audio processing with respect to speaker recognition, speech recognition or any such related task. But you also know that with any of these security technologies, there can be attacks or there can be people who have any kind of ill intention who would like to defraud the system. Right? Who would like to fool the system? Have you heard of any such examples, any such real-world examples where any kind of security system is in place, be it biometrics, face, voice, any of those. And there have been cases where these systems have been fooled. Anybody remembers any such instance? Would like to share. And when you have to speak up. Is it going to be on the... Okay, all right. So there have been several such instances, not only in... I'm audible, right? Somebody please speak up. I'm not able to hear you guys. Yes, please. To see the text and all. Okay, so there have been several such instances with respect to different kinds of tasks, automation tasks that we have been performing. Right? And specifically speech. These are all the news clippings that you see of documented cases. Okay? One of the interesting ones, the one that you see at the bottom, city bank launches, voice passwords in India. Right? So city bank launched voice passwords. You can gain access to accounts by using... By speaking out your passwords. And you will also see in capture, right? That you can speak out what... So there are image captures and then there are audio captures also that are there. For people with special requirements, they can speak out audio captures, right? So... But these can be fooled using some of these technologies. And this is what also happened in some of these cases. For example, this adoboco, they have a Photoshop for voice which can lead to concerns. We have robot speech simulator and several other such things that have been proposed in the literature. And similar counterparts have also been proposed which can be used to defraud. Okay? So specifically speaking, this line of research of presentation attack like we call it or spoofing, this has been in the community for like 15 years or so now. The research started the first competition around this. It's called as the spoofing audio spoofing competition. AASB audio spoofing verification competition we call it. So this competition started I think around 2000 again or so in the research community, right? At that time, they were looking at some very specific kinds of attacks and I'll show you what those kinds of attacks were. But these competitions, when they're still running, those competitions are still running to evaluate the performance or of these attack detection algorithms or manipulation detection algorithms. But after a few years that these research on presentation of spoofing started, ISO got involved, this international standards organization and they created a standards document on presentation attacks. Earlier we used to call it spoofing but then I also termed the test presentation attacks. So anything that is presented in front of a biometric system, it can be attacked. So and then there are different kinds of attacks. So the earlier the nomenclature used to be like real spoof but then they proposed this nomenclature of bonafide and imposter. So bonafide is a real one, right? So in this case with respect to face, if you see bonafide is the real face. So you would enroll a real face, a real face is coming as query. So this is predicted as genuine, right? But then you could have imposters. Now what are those imposter? Imposter is anyone who's trying to fraudulently get access to the system. Now this is something called a zero effort imposter. What is a zero effort imposter? So let's say I want to gain access to your account. I just go in front of the biometric system and I give my biometric. Let's say that biometric is face or that biometric is audio. I just give that biometric as without any effort of trying to look like you or trying to impersonate your voice or fingerprint. Whatever that modality is, I just go using my characteristics and try to gain access to your account. So that is something called a zero effort imposter which is in which case it is very likely that I will not be able to gain access. It will be it will be filled attempt, right? But then there are cases in which one can get access and go back to dodding to zoom. When where we had these goat lamb sheep and wolves, right? There are some cases, there are some individuals who are easy to impersonate. So if these are easy to impersonate people, even with zero effort imposter, I might be able to gain access depending on a lot of characteristics, lot of factors. But then what I can also do is do a bit more effort in trying to gain access to your account. Now in what ways can I try to gain access to your account? Let's say if it is audio in what ways can I get access to your account? Can you think of it? But if I have to spoof an audio data and audio signal, if you were given this task of spoofing, what will you do? How will you create a spoof of audio? Let's say you have to gain access to my account. What will you do? I am waiting for the answers. Try to speak in your language. Please open that account. Okay, so try to speak in my audio. Yes, nearly. The only method is to speak in your form. Like, like, like. Okay, anything else? Record your voice and then feed it. Record my voice and feed it. That is good. What else? Use the text to speak in your form. Use the text to speak in your form. We can also make a deep break of the voice. We can also make a deep break of my voice. All of these are valid answers. And the method of, let me just play this audio signals for you. In order to play this, I think I will have to go to the drive. Okay, let me go to the drive. Just listen to this audio and tell me if it is correct. Okay, so whose voice is this? Charo Khan, obviously. Do you think it is a real voice of Charo Khan or this is an end of AI attack? You cannot hear the audio. We are not able to react. You cannot hear my audio or you cannot hear the audio. I just played. Videos, yes. That audio. You couldn't hear the audio. I played, right? Yes, ma'am. Okay, let me give me one second. Let me stop presenting. Oh, you are here now? That's all it's Shashma. Agalwale, heaven I operation Rehleena. Yes ma'am. Okay. Siddhi Vinay, electronic say, let us smart phone Karithkat. Marked our self-heak-host. Okay. Okay. It is a recorded song. It is a recorded song. How did you record it? It is a recorded song. How did you record it? You mean some gaps in Charo Khan's voice, there is some gap. He is used to speaking with gaps. No? Yes, yes. Anybody else would like to give it a try? Okay, let me play another one. I can say that I am a Mughambo. What about this? Yes ma'am, I think real voice. I am Raj Furi. Okay. No, I don't think it is real. The tonality is a little bit changed. Okay. So both of these guys are generated audio. Okay. Charo Khan as well as Amarish Puri, this Amarish Puri's audio was generated after he passed away. And this Charo Khan audio, so there is this entire video that was generated a few years back by Katbury's. And they did a, they did a, add with Charo Khan's voice for local Kirana stores. Right? Because if you hear the audio, he is talking about local Kirana's store. And they are taking this and pass the shop of Keshav. These guys can't afford Charo Khan. These local Kirana stores and local Angal and anti shops. They can't afford Charo Khan. So they, Katbury's used Genie AI. I think it was Katbury's. They used Genie AI to create this entire ad. And that ad you can find that ad on YouTube. It should be still there. So benefit is you can create. But then I don't know if they took permission from Charo Khan or not. Right? You can create all those sort of things. And I mean, you guys know that we are talking about B-Fix. Still we are having a discussion on whether it is fake or not. Think about any normal person who does not know the concept of B-Fix so well. That how good or how bad it is. How would they differentiate between the two. Right? So there is an entire categorization of spoofing attacks. You can generate samples based on AI methodologies. And using some non AI methodologies as per. So if we talk about obviously these non AI methodologies came earlier. And this when I was talking about the research started like 15 years back. We started with non AI methodologies right? And non AI methodologies therefore 5 kinds of attacks which are more common ones. The first one is your replay right? So replays I think Ravi's Ravi suggested it earlier that what you can do is you have my. You have my audio right now that I am you have so many required audio samples of me from over multiple classes right? If you want to gain access to my account, you can you can take this audio and you can replay it. Whenever take out words or do something of that sort and replay it to gain access to my account right? So that is your replay attack okay. To give you an example of replay with respect to a face. This is how you can actually visualize it. So this is your real real real frames from real video. These are again your frames from real video. But what you can do is you can record this video and replay it. Let's say I was trying to gain access to my KYC or my account or I want to be present in a meeting, put attendance using a face biometric system. You can use that and gain access multiple times using that right? So again several years back okay there these school teachers in some country I think it was Pakistan. So there was this grant coming in from United Nations for school teachers. And they were mandated to put attendance using face biometrics because there was grant coming in for promoting school education in different countries. So they said that we and and salaries of teachers and everybody was going in. They kind of mandated that in order to ensure that teachers go to school every day. So you have to go to school you have to put your you have to take a selfie and you have to post it on some server with with a date and time stamp so that your attendance is recorded right and based on that attendance the this already was released every month right. So so what some of the teachers used to do that what I'll do is today I'll take a picture I'll go to school I'll take five pictures for me okay and what I can do is tomorrow if I want to skip school I have multiple pictures I'll take this picture and I click another picture of this from the phone. Or I'll take a I'll take a printout of this click another picture and upload this so the date and time stamp that comes with the photo in the metadata that will be for tomorrow because I click the photograph from the photograph the the impersonation one that I had that I clicked tomorrow so then then date and time stamp will be fixed and I'll have my attendance marked right the same kind of thing can be done with respect to audio where you can capture these sentences and you can replay it again and again right so today I go with the background and all I can capture the audio and wherever required I can replay the audio to gain access or to provide incorrect information or whatever is the requirement right. So that is your replay attack impersonation impersonation is simple we understand that if you if you try to impersonate someone and this impersonation can be done by different methods earlier this used to be non AI based and in non AI based it would be what would be non AI based impersonation it would be something like like we there are mimicry artists right there mimicry artist who can very well mimic the audio of Shah Rukh Khan Amitabha. So that is the question or or anybody right they can mimic anybody's voice so so that mimicry would be that mimicry if used for impersonation would be considered as an attack and that that would be that that would be impersonation to gain access right the another one is copy move right what is copy move copy move is something where I can take an audio signal and take some parts of an audio from one place to other in the audio right and this I think happened in the elections this time when when we had general elections this is a new clapping I got from YouTube and let me just play this for you you can see the screen the YouTube screen is not shared oh sorry sorry sorry I the screen got unshared okay can you see it now man not able to hear what you are again not able to hear the voice why is it doing this anybody knows I guess instead of sharing the screen you could share the tab that would allow us to hear the audio in the YouTube tab okay let me do that entire screen show where the S.P. and O.B. is the first audio which was claimed to be the original audio. Now let me play you the fake audio. Ranta, Paisi, Sarkar, Banegi, Torek, Gair, Sambhayi, Dhani, S.P. and O.B. is the one who will be the leader of the Dharveh fan. See what happened here? The lift is not thinking with voice. The lift is not with voice. What is the kind of attack we are trying to do here? So what they did in this is they had the original video of Mr. Amit Shah. They did not change the entire audio or video. What they did? In the beginning of the speech they inserted this word Gair Sambhayi Dhani. When he was referring to this they inserted the word Gair Sambhayi Dhani. And then towards the end when he was saying that this reservation will continue. They said he is going to sum up. So in copy move what we do you take a word from somewhere and you insert it. Or you can change phrases. You have a phrase spoken at one place. You pick up this phrase and you insert it somewhere else. Now in this case what they did was this Gair Sambhayi Dhani is probably the word which they might have found. They might not have found anywhere to be in the same speech. Now if they did not find that word in the same speech with the same attire because there is video also. See this is multi model spoofing that is happening. Because you have to spoof the video and you also have to spoof the audio along with it. So lip syncing and everything else has to go hand in hand very well. So if he had spoken the word Gair Sambhayi Dhani anywhere anytime. You can pick up that word and insert it here. But since you have to do a complete multi model spoofing, multi model generation of the attack. You need him to speak. You need a video of him speaking the same thing with lip syncing and everything. And that has to be inserted in the middle and those three barbocchi because the sentence has to flow in place. Earlier when we attacked this copy move we used to do. This used to be a very simple copy paste here and there. Either cut copy paste, cut copy move is something we used to say. So delete it or copy move it here. But now what has happened. And in the earlier days it used to be just. If you have an audio of the person speaking those sentences or words only then you could create this. But today with this generative way I algorithm what you can do is you can also create these complete instances of people speaking. Even if they even if they weren't existing with you. So this is a combination this Amitshaka video that you just saw Amitshaka video plus audio that we saw. It was a combination of copy move along with speech synthesis. Along with speech synthesis that we were able to generate. So not only speech synthesis probably video synthesis also that happened along with it because. If speech synthesis is done so the entire thing has to be shifted further back right. Because words are introduced. So so that is what we did right and then audio splicing is. Audio could like use to splice. So audio splicing would be similar to copy move where copy move you were just shifting out. Audio splicing you can actually remove out or words completely or phrases completely right. So earlier again this used to be all non AI today you can do all of it with AI and what you'll get is. A lot more simplified version and not simplified a lot more smooth out version. Of the attacks right so replay car sakhte hai ho se we can do replay with. Using AI based attacks you can do impersonation you can do copy move audio splice audio splicing using all of using AI generative AI algorithms right. And in AI based the two main categories that you have is you can do speech synthesis. So in speech synthesis what we do in speech synthesis we can have the complete speech synthesized right with you. And you can do it in who so ever voice you want. So did I play you that song of a the first slum and a regime thing sometime in the past. You guys remember. You have to speak out I am not able to see the chat. I remember that there was no means to have had not made any song of particular. You don't remember okay I will search for it and I will I will show it to you okay so you could do speech synthesis so. Generate something completely in like like Amrish puri's audio right so that Amrish puri's audio voice a complete synthesis. We just have we don't we had completely new generation of that audio right and then you could do voice conversion and in voice conversion you can you can take whatever I'm speaking and you can have it present in let's say Shreya's voice or you can have a present in. Shams voice so the complete lecture is being given by any other student be it Shambi it anybody else right so so the entire voice conversion can be done using a I based attacks now this this broad category of being able to impersonate or being able to create new things. Change from one to other this this broad line of attacks is called as deep fake attacks images you have seen a lot of it in the in the. General news and stuff but this is very very much possible very much even prevalent I would say.\n",
            "Process complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def transcribe_video(video_path, output_path=None, model_size=\"base\", language=\"hi\"):\n",
        "    \"\"\"\n",
        "    Transcribe a video file using OpenAI's Whisper model.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the video file to transcribe\n",
        "        output_path (str, optional): Path to save the transcript. If None, uses the video filename with .txt extension\n",
        "        model_size (str, optional): Whisper model size: \"tiny\", \"base\", \"small\", \"medium\", or \"large\"\n",
        "        language (str, optional): Language hint for the transcription model (e.g., \"hi\" for Hindi, \"en\" for English)\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the saved transcript file\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_path):\n",
        "        print(FileNotFoundError(f\"Video file not found: {video_path}\"))\n",
        "\n",
        "    if output_path is None:\n",
        "        output_path = os.path.splitext(video_path)[0] + \".txt\"\n",
        "\n",
        "    print(f\"Loading Whisper {model_size} model...\")\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    print(f\"Transcribing {os.path.basename(video_path)}...\")\n",
        "    with tqdm(total=10, desc=\"Transcribing\", bar_format='{l_bar}{bar}| {elapsed}') as pbar:\n",
        "        result = model.transcribe(\n",
        "            video_path,\n",
        "            task=\"transcribe\",\n",
        "            language=language,\n",
        "            verbose=False\n",
        "        )\n",
        "        pbar.update(1000)\n",
        "\n",
        "    return result['text']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_transcript(text, file_path):\n",
        "    \"\"\"\n",
        "    Removes filler words like \"um\", \"uh\", etc.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text string containing the transcript generated by model\n",
        "\n",
        "    Returns:\n",
        "        cleaned_text (str): Text string after removing all the filler words\n",
        "    \"\"\"\n",
        "    filler_words = [r\"\\bum\\b\", r\"\\buh\\b\", r\"\\blike\\b\", r\"\\buhm\\b\", r\"\\buhhmm\\b\", r\"\\ba\\b\", r\"\\bhmm\\b\"]\n",
        "    pattern = re.compile(\"|\".join(filler_words), flags=re.IGNORECASE)\n",
        "\n",
        "    cleaned_text = pattern.sub(\"\", text)\n",
        "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "\n",
        "    os.makedirs(file_path, exist_ok=True)\n",
        "\n",
        "    with open(file_path+'cleaned_transcript.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(cleaned_text)\n",
        "    print(f\"Transcript cleaned and saved to: {file_path}cleaned_transcript.txt\")\n",
        "\n",
        "    return file_path\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    video_path = '/data/video_lecture/speech_recording.mp4'\n",
        "    target_dir = 'data/transcripts/'\n",
        "    model = \"base\"\n",
        "\n",
        "    if os.path.exists(video_path):\n",
        "\n",
        "        transcript = transcribe_video(video_path, model_size=model)\n",
        "        cleaned_transcript_path = preprocess_transcript(transcript, target_dir)\n",
        "        print(transcript)\n",
        "        print(f\"Process complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYklgq4IWG8t"
      },
      "source": [
        "## Audio Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI4Zb2EMlMWt",
        "outputId": "70db7e26-7463-4b48-dc4d-7b96b1810b06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:parler_tts.modeling_parler_tts:Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
            "  \"_name_or_path\": \"google/flan-t5-large\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2816,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 24,\n",
            "  \"num_heads\": 16,\n",
            "  \"num_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "WARNING:parler_tts.modeling_parler_tts:Config of the audio_encoder: <class 'transformers.models.dac.modeling_dac.DacModel'> is overwritten by shared audio_encoder config: DacConfig {\n",
            "  \"_name_or_path\": \"ylacombe/dac_44khz\",\n",
            "  \"architectures\": [\n",
            "    \"DacModel\"\n",
            "  ],\n",
            "  \"codebook_dim\": 8,\n",
            "  \"codebook_loss_weight\": 1.0,\n",
            "  \"codebook_size\": 1024,\n",
            "  \"commitment_loss_weight\": 0.25,\n",
            "  \"decoder_hidden_size\": 1536,\n",
            "  \"downsampling_ratios\": [\n",
            "    2,\n",
            "    4,\n",
            "    8,\n",
            "    8\n",
            "  ],\n",
            "  \"encoder_hidden_size\": 64,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"hop_length\": 512,\n",
            "  \"model_type\": \"dac\",\n",
            "  \"n_codebooks\": 9,\n",
            "  \"quantizer_dropout\": 0.0,\n",
            "  \"sampling_rate\": 44100,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"upsampling_ratios\": [\n",
            "    8,\n",
            "    8,\n",
            "    4,\n",
            "    2\n",
            "  ]\n",
            "}\n",
            "\n",
            "WARNING:parler_tts.modeling_parler_tts:Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
            "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini-v2-empty/decoder\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_cross_attention\": true,\n",
            "  \"architectures\": [\n",
            "    \"ParlerTTSForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1025,\n",
            "  \"codebook_weights\": null,\n",
            "  \"cross_attention_implementation_strategy\": null,\n",
            "  \"delay_strategy\": \"delay\",\n",
            "  \"dropout\": 0.1,\n",
            "  \"eos_token_id\": 1024,\n",
            "  \"ffn_dim\": 4096,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_factor\": 0.02,\n",
            "  \"is_decoder\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"parler_tts_decoder\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_codebooks\": 9,\n",
            "  \"num_cross_attention_key_value_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 16,\n",
            "  \"pad_token_id\": 1024,\n",
            "  \"rope_embeddings\": false,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"scale_embedding\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_fused_lm_heads\": true,\n",
            "  \"vocab_size\": 1088\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ENGLISH TO MARATHI TRANSLATION\n",
        "\n",
        "from deep_translator import GoogleTranslator\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import re\n",
        "from langdetect import detect\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-parler-tts\")\n",
        "description_tokenizer = AutoTokenizer.from_pretrained(model.config.text_encoder._name_or_path)\n",
        "\n",
        "description = \"Sanjay who speaks fluent Marathi language delivers a slightly expressive and animated speech with a moderate speed and pitch. The recording is of very high quality, with the speaker's voice sounding clear and very close up.\"\n",
        "description_input_ids = description_tokenizer(description, return_tensors=\"pt\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "o188NSfyT1Xc"
      },
      "outputs": [],
      "source": [
        "del model, chunks, translation, tokenizer, description_tokenizer, description_input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLBJMFBc9Oqz",
        "outputId": "e973c062-d3a2-4a55-9a54-5e8d2ff08383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "आणि विशेषत: भाषण. ही सर्व बातम्या आहेत ज्या आपण दस्तऐवजीकृत प्रकरणांची पाहता. विशेष आवश्यकतांसह, ते ऑडिओ कॅप्चर बोलू शकतात, बरोबर? म्हणून ... परंतु यापैकी काही तंत्रज्ञानाचा वापर करून हे फसवले जाऊ शकते. आणि यापैकी काही प्रकरणांमध्ये असेही घडले आहे. उदाहरणार्थ, या अ‍ॅडोबोकोने आवाजासाठी फोटोशॉप केला आहे ज्यामुळे रोबोट भाषण सिम्युलेटर आणि अशा अनेक गोष्टींचा वापर केला गेला आहे.यापूर्वी आम्ही याला स्पूफिंग म्हणायचो पण नंतर मी चाचणी सादरीकरणाचे हल्ले देखील म्हटले होते. बायोमेट्रिक सिस्टमच्या समोर सादर केलेले काहीही, त्यावर हल्ला केला जाऊ शकतो. फेस. म्हणून आपण वास्तविक चेहरा नोंदवाल, वास्तविक चेहरा क्वेरी म्हणून येत आहे. म्हणून हे अस्सल म्हणून अंदाज आहे, बरोबर? बायोमेट्रिक. लेट असे म्हणतात की बायोमेट्रिक चेहरा आहे किंवा बायोमेट्रिक ऑडिओ आहे.मी फक्त त्या बायोमेट्रिकला देतो की आपण पाहण्याचा प्रयत्न केला नाही किंवा आपला आवाज किंवा फिंगरप्रिंटची तोतयागिरी करण्याचा प्रयत्न केला नाही. जे काही बदल आहे, मी फक्त माझ्या वैशिष्ट्यांचा वापर करून आपल्या खात्यात प्रवेश मिळविण्याचा प्रयत्न करतो. शून्य प्रयत्न इम्पोस्टर असे काहीतरी आहे जे मला प्रवेश मिळू शकणार नाही. झूम. जेव्हा आमच्याकडे या बकरीच्या कोकरू मेंढ्या आणि लांडगे होते, बरोबर? काही घटना घडल्या आहेत, अशी काही व्यक्ती आहेत ज्यांना तोतयागिरी करणे सोपे आहे. लोकांची तोतयागिरी करणे सोपे असेल तर, शून्य प्रयत्नांनुसार, बर्‍याच गोष्टींवर अवलंबून मी प्रवेश करू शकतो. ऑडिओ आपल्या खात्यात कोणत्या प्रकारे प्रवेश मिळवू शकतो?आपण याचा विचार करा? परंतु जर मला ऑडिओ डेटा आणि ऑडिओ सिग्नलचा स्पूफ करावा लागला असेल तर आपल्याला स्पूफिंगचे हे कार्य दिले गेले असेल तर आपण काय कराल? आपण ऑडिओची फसवणूक कशी कराल? आपण माझ्या खात्यात प्रवेश घ्यावा लागेल असे म्हणू शकता. मी उत्तराची वाट पाहत आहे. आपल्या भाषेत बोलण्याचा प्रयत्न करा. .ओके, आणखी काही? आपला आवाज रेकॉर्ड करा आणि नंतर त्यास खायला द्या. माझा आवाज पुन्हा करा आणि ते चांगले आहे. हे चांगले आहे. दुसरे काय? आपल्या फॉर्ममध्ये बोलण्यासाठी मजकूर वापरा. ​​आपल्या फॉर्ममध्ये बोलण्यासाठी मजकूर वापरा. ​​आम्ही माझ्या आवाजाचा खोल ब्रेक देखील करू शकतो. या सर्व गोष्टी मला वाटू शकतात, मला असे वाटते की, मी या गोष्टीची पूर्तता करू शकतो. ड्राइव्हवर जा. फक्त हा ऑडिओ ऐका आणि ते योग्य आहे की नाही ते सांगा.आपणास असे वाटते की हा चारो खानचा वास्तविक आवाज आहे किंवा हा एआय हल्ल्याचा शेवट आहे? आपण ऑडिओ ऐकू शकत नाही. आम्ही प्रतिक्रिया देऊ शकत नाही. आपण माझा ऑडिओ ऐकू शकत नाही किंवा आपण ऑडिओ ऐकू शकत नाही. Shashma.agalwale, स्वर्ग मी ऑपरेशन रेहलीना.यस मॅम.ओकाय.सिड्धी विनय, इलेक्ट्रॉनिक म्हणा, आपण स्मार्ट फोन करिथकॅट.आणि आमच्या सेल्फ-हिक-होस्ट.ओके. gaps.no? होय, होय. इतर कोणीही ते देण्याचा प्रयत्न करेल? ठीक आहे, मला आणखी एक खेळू दे. मी असे म्हणू शकतो की मी मुघॅम्बो आहे. हे काय आहे? होय मॅम, मला वाटते की वास्तविक आवाज आहे.ठीक आहे.चरो खान तसेच अमरीश पुरी, हे अमरीश पुरी यांचे ऑडिओ निधन झाल्यानंतर तयार झाले. केशव. हे लोक चारो खान परवडत नाहीत. हे स्थानिक किराणा स्टोअर्स आणि स्थानिक अंगल आणि अँटी शॉप्स. त्यांना चारो खान परवडत नाही. ते, कॅटबरीचे वापरलेले जिनी आय. मला असे वाटते की ते कॅटबरीचे होते. आणि जर आपण या गोष्टीचा उपयोग केला असेल तर मला ते तयार होऊ शकते. चारो खान किंवा नाही.बी-फिक्सची संकल्पना इतक्या चांगल्या प्रकारे माहित नसलेल्या कोणत्याही सामान्य व्यक्तीचा विचार करा. हे किती चांगले किंवा किती वाईट आहे. ते दोन दरम्यान कसे फरक करतात? तर स्पूफिंग हल्ल्यांचे संपूर्ण वर्गीकरण आहे. एआय पद्धतींबद्दल आपण काही नॉन-पद्धतींचा वापर केला तर मी या पद्धतीनुसार चर्चा केली. एआय पद्धती बरोबर? आणि एआय नसलेल्या पद्धती म्हणून 5 प्रकारचे हल्ले जे अधिक सामान्य आहेत. प्रथम एक आपला रीप्ले बरोबर आहे? म्हणून मला असे वाटते की रवीच्या रवीने पूर्वी असे सुचवले की आपण माझे ऑडिओ आहे की आपण माझ्या एकाधिक वर्गांद्वारे माझे बरेच आवश्यक आहे की आपण हे जाणून घेऊ शकता की आपण हे जाणून घेऊ शकता की आपण हे जाणून घेऊ शकता की आपण हे जाणून घेऊ शकता की आपण हे जाणून घेऊ शकता की आपण हे जाणून घेऊ शकता की आपण हे जाणून घेऊ शकता की आपण हे जाणून घेऊ शकता.जेव्हा जेव्हा शब्द बाहेर काढतात किंवा त्या क्रमवारीत काही करा आणि माझ्या खात्यात प्रवेश मिळविण्यासाठी ते पुन्हा प्ले करा तेव्हा आपला रीप्ले हल्ला ठीक आहे. समोरासमोरील रीप्लेचे एक उदाहरण द्या. हे आपण प्रत्यक्षात कसे दृश्यमान करू शकता हे आहे की हे आपले वास्तविक वास्तविक फ्रेम आहे. आपण हे जाणून घेऊ शकता की आपण हे जाणून घेऊ शकता की आपण हे जाणून घेऊ शकता की आपण हे जाणून घेऊ शकता की आपण हे व्हिडिओ बनवू शकता. बैठकीत उपस्थित राहण्यासाठी, फेस बायोमेट्रिक सिस्टमचा वापर करून उपस्थिती ठेवा. आपण त्या अधिकाराचा वापर करून अनेक वेळा प्रवेश करू शकता? म्हणून पुन्हा कित्येक वर्षांपूर्वीच या शालेय शिक्षकांना काही देशात मला असे वाटते की ते पाकिस्तान होते. शालेय शिक्षकांसाठी संयुक्त राष्ट्रांकडून हे अनुदान देण्यात आले होते. आणि त्यांना उपस्थित राहण्याचे आदेश देण्यात आले होते कारण तेथे विभागातील शिक्षण देण्यास मदत केली गेली होती.म्हणून ते म्हणाले की आम्ही आणि शिक्षकांचे पगार आणि प्रत्येकजण आत जात आहे. शिक्षक दररोज शाळेत जावेत हे सुनिश्चित करण्यासाठी. तुम्हाला शाळेत जावे लागेल, तुम्हाला तुमचा सेल्फी घ्यावा लागेल आणि तुम्हाला काही सर्व्हरवर पोस्ट करावे लागेल की मी दर महिन्याला जे काही केले आहे तेवढेच मी जे काही केले आहे तेच आहे की मी हेच केले आहे की मी हेच केले आहे की मी हेच केले आहे की मी हेच केले आहे की मी हेच केले आहे की मी हेच केले आहे की मी हेच केले आहे की मी हेच केले आहे की मी हेच केले आहे की मी या गोष्टीचा वापर केला आहे. शाळेत जा मी माझ्यासाठी पाच छायाचित्रे घेईन ठीक आहे आणि मी काय करू शकतो उद्या मला शाळा वगळायची असेल तर माझ्याकडे अनेक चित्रे असतील तर मी हे चित्र घेईन आणि मी फोनवरून याचे आणखी एक चित्र क्लिक करा.किंवा मी घेईन मी या क्लिकचे आणखी एक चित्र घेईन आणि हे अपलोड करीन म्हणून मेटाडेटामधील फोटोसह येणारी तारीख आणि वेळ स्टॅम्प उद्या असेल कारण मी उद्या क्लिक केलेल्या छायाचित्रातील छायाचित्रांवर क्लिक करतो, मग मी पुन्हा क्लिक केले असेल तर मी पुन्हा एकदा या गोष्टीचा विचार केला जाऊ शकतो आणि आपण पुन्हा या गोष्टीचा विचार केला जाऊ शकतो आणि आपण पुन्हा या गोष्टीचा विचार केला जाऊ शकतो आणि आपण हेच केले जाऊ शकते आणि आपण हेच केले आहे की आपण हेच केले आहे आणि आपण हेच ऑडिओ करू शकता म्हणून आपण पुन्हा प्रयत्न करू शकता आणि आपण हेच केले आहे की आपण हे ऑडिओ करू शकता आणि आपण पुन्हा प्रयत्न करू शकता म्हणून आपण हेच केले आहे आणि आपण हेच केले आहे की आपण हे ऑडिओ केले जाऊ शकता आणि आपण पुन्हा प्रयत्न करू शकता आणि आपण हेच केले आहे की आपण हेच केले आहे आणि आपण पुन्हा प्रयत्न करू शकता आणि आपण हेच केले आहे की आपण हेच केले आहे आणि आपण पुन्हा प्रयत्न करू शकता आणि आपण हेच केले आहे आणि आपण हे ऑडिओ करू शकता म्हणून आपण पुन्हा प्रयत्न करू शकता. मी ऑडिओ कॅप्चर करू शकतो आणि जिथे आवश्यक असेल तेथे मी प्रवेश मिळविण्यासाठी किंवा चुकीची माहिती किंवा जे काही योग्य आहे ते प्रदान करण्यासाठी ऑडिओ पुन्हा प्ले करू शकतो.म्हणूनच आपला रीप्ले हल्ला तोतयागिरीची तोतयागिरी सोपा आहे हे आम्हाला समजले आहे की जर आपण एखाद्याची तोतयागिरी करण्याचा प्रयत्न केला तर आणि ही तोतयागिरी वेगवेगळ्या पद्धतींनी केली जाऊ शकते जर हे पूर्वी एआय आधारित असायचे आणि एआय आधारित नसलेले ते असे काहीतरी असेल जे तेथेच एक मिमिक्री कलाकार आहेत जे फारच मिटोच मिटो मिटोच आहे.आता मी तुम्हाला बनावट ऑडिओ.रंटा, पेसी, सरकार, बणेगी, तोरेक, गायर, सांभाय, धनी, एस.पी. आणि ओ.बी. जो धारवेह चाहत्यांचा नेता असेल तोच आहे. येथे काय घडले ते पहा. लिफ्ट आवाजाने विचार करत नाही. लिफ्ट आवाजाने नाही. आम्ही येथे काय करण्याचा प्रयत्न करीत आहोत? मग त्यांनी श्री. अमित शाहचा मूळ व्हिडिओ बदलला नाही. या संदर्भात त्यांनी गैर संभाय धनी हा शब्द घातला. आणि शेवटी जेव्हा तो असे म्हणत होता की हे आरक्षण सुरूच राहील. ते म्हणाले की, तो सांगत आहे. कारण आपण कुठेतरी काय बोलता ते कॉपी करा आणि आपण या ठिकाणी जे काही केले आहे की आपण या ठिकाणी काय केले आहे. संभाय धनी हा कदाचित हा शब्द आहे जो त्यांना सापडला असेल.त्यांना कदाचित एकाच भाषणात कोठेही सापडले नसेल. आता त्यांना त्याच भाषणात त्याच भाषणात हा शब्द सापडला नाही कारण तेथे व्हिडिओ आहे. हे मल्टी मॉडेल स्पूफिंग आहे. कधीही. आपण हा शब्द निवडू शकता आणि येथे येथे घालू शकता. परंतु आपल्याला संपूर्ण मल्टी मॉडेल स्पूफिंग करावे लागले आहे, हल्ल्याची मल्टी मॉडेल निर्मिती. आपल्याला त्याला बोलण्याची आवश्यकता आहे. आपल्याला ओठांच्या समक्रमणासह समान गोष्ट बोलण्याची आवश्यकता आहे. आणि त्या तीन बार्बोचीने वापरल्या पाहिजेत. आणि तेथे एक कट कॉपी पेस्ट, कट कॉपी मूव्ह ही एक गोष्ट आहे जी आम्ही म्हणायचं.परंतु आता जे घडले आहे. आणि पूर्वीच्या दिवसांत ते फक्त असायचे. जर आपल्याकडे त्या वाक्ये किंवा शब्द बोलणार्‍या व्यक्तीचा ऑडिओ असेल तर आपण हे तयार करू शकाल. परंतु आज या जनरेटिव्ह मार्गाने आपण काय करू शकता हे आपण लोक बोलण्याची ही संपूर्ण घटना देखील तयार करू शकता. जरी ते आपल्याशी जोडले गेले असले तरी ते एक व्हिडिओ आहे. स्पीच संश्लेषणासह कॉपी हलवा. भाषण संश्लेषणासह आम्ही तयार करण्यास सक्षम होतो. म्हणूनच केवळ भाषण संश्लेषणच नाही की कदाचित त्याबरोबरच घडले कारण. भाषण संश्लेषण केले गेले असेल तर संपूर्ण गोष्ट पुन्हा बदलली जावी. कारण आपण ज्या गोष्टीचा वापर केला आहे तेच होते की आम्ही ऑडिओ स्प्लिटोचा वापर केला आहे. बाहेर जात आहे.ऑडिओ स्प्लिसिंग आपण प्रत्यक्षात काढू शकता किंवा शब्द पूर्णपणे किंवा वाक्ये पूर्णपणे योग्य करू शकता. म्हणून पुन्हा हे सर्व आज सर्व नॉन एआय असायचे की आपण हे सर्व एआय सह करू शकता आणि आपल्याला जे मिळेल ते आहे. अधिक सुलभ आवृत्ती आहे आणि हल्ल्यांद्वारे आपण पुन्हा पुन्हा प्रयत्न करू शकता की आपण पुन्हा एकदा स्प्लिसिंग करू शकता. एआय जनरेटिव्ह एआय अल्गोरिदम योग्य वापरणे आणि एआय मध्ये आपल्याकडे असलेल्या दोन मुख्य श्रेणी आहेत ज्या आपण भाषण संश्लेषण करू शकता. भाषण संश्लेषणात आम्ही भाषण संश्लेषणात काय करतो ते आपल्याबरोबर संपूर्ण भाषण संश्लेषित करू शकते. आणि आपण जे काही बोलतो आहे की मी जे काही बोलतो आहे की मी काय बोललो आहे. चॅट. मला आठवते की विशिष्ट गीताचे कोणतेही गाणे तयार केले नव्हते.तुला आठवत नाही मी ठीक आहे मी त्याचा शोध घेईन आणि मी ते तुला ठीक आहे हे मी तुला दाखवीन जेणेकरून आपण भाषण संश्लेषण करू शकाल. अमृत पुरीच्या ऑडिओमध्ये काहीतरी पूर्णपणे तयार करा जेणेकरून अमृत पुरीचा ऑडिओ व्हॉईस पूर्ण संश्लेषण करा. आम्ही त्या ऑडिओची पूर्णपणे नवीन पिढी केली नाही आणि मग आपण जे काही सांगू शकता की आपण जे काही सांगू शकता की आपण जे काही सांगू शकता की आपण असे करू शकता की आपण जे काही सांगू शकता की आपण असे करू शकता की आपण जे काही सांगू शकता की आपण असे करू शकता की आपण जे काही सांगू शकता की आपण असे करू शकता की आपण जे काही करू शकता आणि आपण असे म्हणू शकता इन.शॅम व्हॉईस म्हणून संपूर्ण व्याख्यान इतर कोणत्याही विद्यार्थ्याने दिले आहे तर ते शांबी असेल तर ते इतर कोणासही योग्य आहे म्हणून संपूर्ण व्हॉईस रूपांतरण मी आधारित हल्ल्यांचा वापर करून केले जाऊ शकते आता ही विस्तृत श्रेणी तोतयागिरी करण्यास सक्षम आहे किंवा नवीन गोष्टी तयार करण्यास सक्षम आहे. या हल्ल्याची ही विस्तृत गोष्ट आहे परंतु ती फारच जास्त होती म्हणून आपण हे पाहिले आहे की हे फारच जास्त आहे. म्हणा.\n"
          ]
        }
      ],
      "source": [
        "def split_text(text, num_chunks=4):\n",
        "    \"\"\"Splits text into roughly equal chunks without breaking sentences or words.\"\"\"\n",
        "\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', text)\n",
        "    target_length = 1000\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    current_length = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if current_length + len(sentence) <= target_length:\n",
        "            current_chunk += sentence\n",
        "            current_length += len(sentence)\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence\n",
        "            current_length = len(sentence)\n",
        "\n",
        "    chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def translate_to_marathi(english_text):\n",
        "    \"\"\"\n",
        "    Translates English text to Marathi using the Deep Translate model.\n",
        "\n",
        "    Args:\n",
        "        english_text: The English text to translate.\n",
        "\n",
        "    Returns:\n",
        "        The translated Marathi text.\n",
        "    \"\"\"\n",
        "    translator = GoogleTranslator(source='en', target='mr')\n",
        "    translated_text = translator.translate(english_text)\n",
        "    return translated_text\n",
        "\n",
        "\n",
        "with open('/data/transcripts/cleaned_transcript.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "chunks = split_text(lines[0], num_chunks=4)\n",
        "marathi_translation = \"\"\n",
        "\n",
        "for chunk in chunks:\n",
        "    translation = translate_to_marathi(chunk)\n",
        "    if detect(translation) != 'en':\n",
        "        marathi_translation += translation\n",
        "\n",
        "print(marathi_translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiDrKE9dzGPi",
        "outputId": "b39e6fd9-58ab-49da-f704-93cdbec17900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output file translated from English to Marathi is saved at /content/data/marathi_tts_out.wav\n"
          ]
        }
      ],
      "source": [
        "# TEXT TO AUDIO CONVERSION\n",
        "\n",
        "max_length = 500\n",
        "chunks = [marathi_translation[i:i + max_length] for i in range(0, len(marathi_translation), max_length)]\n",
        "\n",
        "audio_arr_full = []\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "\n",
        "    prompt_input_ids = tokenizer(chunk, return_tensors=\"pt\").to(device)\n",
        "    generation = model.generate(\n",
        "        input_ids=description_input_ids.input_ids,\n",
        "        attention_mask=description_input_ids.attention_mask,\n",
        "        prompt_input_ids=prompt_input_ids.input_ids,\n",
        "        prompt_attention_mask=prompt_input_ids.attention_mask\n",
        "    )\n",
        "    audio_arr_full.extend(generation.cpu().numpy().squeeze())\n",
        "\n",
        "    if i%5 == 0:\n",
        "        sf.write(\"/data/marathi_tts_out.wav\", audio_arr_full, model.config.sampling_rate)\n",
        "        print('Output file translated from English to Marathi is saved at /data/marathi_tts_out.wav')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
